{"paragraphs":[{"text":"%angular\n<script>\n    function openSnappyPulse(){\n      var proto = window.location.protocol;\n      var currHost = window.location.host;\n      var pulseUrl = proto + '//' + currHost.substring(0, currHost.indexOf(':') + 1) + '5050/dashboard/';\n      window.open(pulseUrl, \"_blank\");\n    }\n</script>\n\n<div style=\"background-color:whitesmoke;\">\n<br>\n <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span>\n<br>\n<a href=\"#/notebook/2ETF88QQF\">Zeppelin Notebook Manager</a>\n<br>\n    \n<h4>Quickstarts</h4>\n&ensp;<a href=\"#/notebook/quickstart\" >Using Spark Scala API</a>\n<br>\n&ensp;<a href=\"#/notebook/2EVF37179\" >Using SQL</a>\n<br>\n&ensp;<a href=\"#/notebook/performance\">Performance Benchmark</a>\n<br>\n\n<h4><b>External Data Sources </b></h4>\n&ensp;<a href=\"#/notebook/2EU4EJDHJ\">Load from External Data Sources</a>\n<br>\n&ensp;<a href=\"#/notebook/2EU6ZXZQJ\">Manage Connectors</a>\n<br>\n<!-- &ensp;<a href=\"#/notebook/2EUZ88BBY\">Data transformation examples</a>-->\n&ensp;<b><i>Data transformation examples</i></b>\n<br>\n\n<h4> Structured Streaming </h4>\n&ensp;<a href=\"#/notebook/2EUCAD6QP\">Example using file source</a>\n\n<h4> Demos with Big Datasets</h4>\n&ensp;<a href=\"#/notebook/airlineanalytics\">Airline Analytics Demo</a>\n<br>\n&ensp;<a href=\"#/notebook/nyctaxianalytics\">NYC Taxi Analytics Demo</a>\n<br>\n&ensp;<a href=\"#/notebook/2DKKFJNZR\">Performance Benchmark(S3 dataset)</a>\n<br>\n\n<h4>References</h4>\n&ensp;<a href=\"https://tibco-computedb.readthedocs.io/en/enterprise_docv1.2/\" target=\"_blank\">ComputeDB documentation</a>\n<br>\n&ensp;<a href=\"https://zeppelin.apache.org/docs/0.8.2/index.html\" target=\"_blank\">Zeppelin Documentation</a>\n</div>\n","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<script>\n    function openSnappyPulse(){\n      var proto = window.location.protocol;\n      var currHost = window.location.host;\n      var pulseUrl = proto + '//' + currHost.substring(0, currHost.indexOf(':') + 1) + '5050/dashboard/';\n      window.open(pulseUrl, \"_blank\");\n    }\n</script>\n\n<div style=\"background-color:whitesmoke;\">\n<br>\n <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span>\n<br>\n<a href=\"#/notebook/2ETF88QQF\">Zeppelin Notebook Manager</a>\n<br>\n    \n<h4>Quickstarts</h4>\n&ensp;<a href=\"#/notebook/quickstart\" >Using Spark Scala API</a>\n<br>\n&ensp;<a href=\"#/notebook/2EVF37179\" >Using SQL</a>\n<br>\n&ensp;<a href=\"#/notebook/performance\">Performance Benchmark</a>\n<br>\n\n<h4><b>External Data Sources </b></h4>\n&ensp;<a href=\"#/notebook/2EU4EJDHJ\">Load from External Data Sources</a>\n<br>\n&ensp;<a href=\"#/notebook/2EU6ZXZQJ\">Manage Connectors</a>\n<br>\n<!-- &ensp;<a href=\"#/notebook/2EUZ88BBY\">Data transformation examples</a>-->\n&ensp;<b><i>Data transformation examples</i></b>\n<br>\n\n<h4> Structured Streaming </h4>\n&ensp;<a href=\"#/notebook/2EUCAD6QP\">Example using file source</a>\n\n<h4> Demos with Big Datasets</h4>\n&ensp;<a href=\"#/notebook/airlineanalytics\">Airline Analytics Demo</a>\n<br>\n&ensp;<a href=\"#/notebook/nyctaxianalytics\">NYC Taxi Analytics Demo</a>\n<br>\n&ensp;<a href=\"#/notebook/2DKKFJNZR\">Performance Benchmark(S3 dataset)</a>\n<br>\n\n<h4>References</h4>\n&ensp;<a href=\"https://tibco-computedb.readthedocs.io/en/enterprise_docv1.2/\" target=\"_blank\">ComputeDB documentation</a>\n<br>\n&ensp;<a href=\"https://zeppelin.apache.org/docs/0.8.2/index.html\" target=\"_blank\">Zeppelin Documentation</a>\n</div>"}]},"apps":[],"jobName":"paragraph_1576643959865_-1604677009","id":"20191205-095258_1521295227","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18264"},{"title":"Simple examples showcasing transformations in ComputeDB","text":"%angular\n<div style=\"background-color:whitesmoke;\">\n<h2>Simple examples showcasing transformations in ComputeDB</h2>\n<ul>\n<li>Paragraphs in this notebook showcase simple transformation examples. Typically, you would define external tables on your remote datasets.<br>Then, use one or more such transformations to shape the data before possibly registering as a View or stored into a in-memory table.</li>\n<li> Note that nested data types are only supported in in-memory column tables.</li>\n<li>We mix scala based Spark Dataframe API + SQL. But, use the special 'Exec scala ..' SQL command to pass Scala code to the cluster using JDBC. Please refer to the <a href=\"#/notebook/quickstart\"> Scala API quickstart</a> for examples.</li>\n<li> Monitor the Cluster, check memory consumption using <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span> <br>The Dashboard tab can be used to monitor the cluster and check the memory consumed by the samples.</li>\n</ul>\n\n\n<h3>Transformation examples in this notebook </h3>\n<ol>\n<li>Flattening complex JSON, XML or Avro objects</li>\n<li>Flattening Array objects</li>\n<li>Add, remove, rename columns</li>\n<li>Pivot a column</li>\n<!-- <li>Featurize your columns</li> -->\n</ol>\nRecommend going through this <a href=\"https://docs.databricks.com/delta/data-transformation/complex-types.html#transforming-complex-data-types-sql-notebook\" target=\"_blank\">third party notebook</a> for a rich set of examples.\n<h5 style=\"color:red;font-weight:bold\">Anonymous user needs to clone this notebook to execute the paragraphs. Clone option available in the pane next to name of the notebook at the top of the page.</h5>\n</div>\n","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":10,"editorMode":"ace/mode/undefined","editorHide":true,"fontSize":9,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<div style=\"background-color:whitesmoke;\">\n<h2>Simple examples showcasing transformations in ComputeDB</h2>\n<ul>\n<li>Paragraphs in this notebook showcase simple transformation examples. Typically, you would define external tables on your remote datasets.<br>Then, use one or more such transformations to shape the data before possibly registering as a View or stored into a in-memory table.</li>\n<li> Note that nested data types are only supported in in-memory column tables.</li>\n<li>We mix scala based Spark Dataframe API + SQL. But, use the special 'Exec scala ..' SQL command to pass Scala code to the cluster using JDBC. Please refer to the <a href=\"#/notebook/quickstart\"> Scala API quickstart</a> for examples.</li>\n<li> Monitor the Cluster, check memory consumption using <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span> <br>The Dashboard tab can be used to monitor the cluster and check the memory consumed by the samples.</li>\n</ul>\n\n\n<h3>Transformation examples in this notebook </h3>\n<ol>\n<li>Flattening complex JSON, XML or Avro objects</li>\n<li>Flattening Array objects</li>\n<li>Add, remove, rename columns</li>\n<li>Pivot a column</li>\n<!-- <li>Featurize your columns</li> -->\n</ol>\nRecommend going through this <a href=\"https://docs.databricks.com/delta/data-transformation/complex-types.html#transforming-complex-data-types-sql-notebook\" target=\"_blank\">third party notebook</a> for a rich set of examples.\n<h5 style=\"color:red;font-weight:bold\">Anonymous user needs to clone this notebook to execute the paragraphs. Clone option available in the pane next to name of the notebook at the top of the page.</h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1576643959868_-404152934","id":"20191127-155550_352738939","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18265"},{"title":"Utility functions - RUN THIS FIRST","text":"%snappydata.sql\nexec scala\n\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\ndef flattenDataframe(df: DataFrame): DataFrame = {\n\nval fields = df.schema.fields\nval fieldNames = fields.map(x => x.name)\nval length = fields.length\n\nfor(i <- 0 to fields.length-1){\n  val field = fields(i)\n  val fieldtype = field.dataType\n  val fieldName = field.name\n  fieldtype match {\n    case arrayType: ArrayType =>\n      val fieldNamesExcludingArray = fieldNames.filter(_!=fieldName)\n      val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(s\"explode_outer($fieldName) as $fieldName\")\n     // val fieldNamesToSelect = (fieldNamesExcludingArray ++ Array(s\"$fieldName.*\"))\n      val explodedDf = df.selectExpr(fieldNamesAndExplode:_*)\n      return flattenDataframe(explodedDf)\n    case structType: StructType =>\n      val childFieldnames = structType.fieldNames.map(childname => fieldName +\".\"+childname)\n      val newfieldNames = fieldNames.filter(_!= fieldName) ++ childFieldnames\n      val renamedcols = newfieldNames.map(x => (col(x.toString()).as(x.toString().replace(\".\", \"_\"))))\n     val explodedf = df.select(renamedcols:_*)\n      return flattenDataframe(explodedf)\n    case _ =>\n  }\n}\ndf\n}\n\nimport org.apache.commons.io.FileUtils\n\ndef deleteFile(path : String): Boolean = {\n\n  FileUtils.deleteQuietly(new java.io.File(path))\n}\n\n\n","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"tableHide":true,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":10,"editorMode":"ace/mode/sql","editorHide":true,"fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"C0":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"C0\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nflattenDataframe: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nimport org.apache.commons.io.FileUtils\ndeleteFile: (path: String)Boolean\n"}]},"apps":[],"jobName":"paragraph_1576643959869_-1987790848","id":"20191108-110446_1457197776","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18266"},{"title":"(1) Scala - Flattening complex JSON, XML or Avro objects","text":"%snappydata.sql\nexec scala\n\nval jsonString = \"\"\"[\n{\n\t\"id\": \"0001\",\n\t\"type\": \"donut\",\n\t\"name\": \"Cake\",\n\t\"image\":\n\t\t{\n\t\t\t\"url\": \"images/0001.jpg\",\n\t\t\t\"width\": 200,\n\t\t\t\"height\": 200\n\t\t},\n\t\"thumbnail\":\n\t\t{\n\t\t\t\"url\": \"images/thumbnails/0001.jpg\",\n\t\t\t\"width\": 32,\n\t\t\t\"height\": 32\n\t\t}\n},\n{\n\t\"id\": \"0002\",\n\t\"type\": \"bread\",\n\t\"name\": \"Cake\",\n\t\"image\":\n\t\t{\n\t\t\t\"url\": \"images/0001.jpg\",\n\t\t\t\"width\": 200,\n\t\t\t\"height\": 200\n\t\t},\n\t\"thumbnail\":\n\t\t{\n\t\t\t\"url\": \"images/thumbnails/0001.jpg\",\n\t\t\t\"width\": 32,\n\t\t\t\"height\": 32\n\t\t}\n}\n]\n\"\"\"\nval rdd = sc.parallelize(Seq(jsonString))\nval df = snappy.read.json(rdd)\n\ndf.printSchema\ndf.count\n\nval df1 = flattenDataframe(df)\ndf1.printSchema\ndf1.count\ndf1.show\n\n// Register your DF as a temp table, and use 'select * from tempTable' to visualize the results.\nsnappy.dropTable(\"flattenedTable\", true)\ndf1.write.format(\"column\").saveAsTable(\"flattenedTable\")\n;\nselect * from flattenedTable","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":10,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":374,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"id":"string","name":"string","type":"string","image_height":"string","image_url":"string","image_width":"string","thumbnail_height":"string","thumbnail_url":"string","thumbnail_width":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576643959870_721600027","id":"20191127-162502_912001224","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18267"},{"title":"(2) Scala + SQL : Flatten Array objects (Json, XML, Avro) ","text":"%snappydata.sql\nexec scala\n/***/\nval jsonString = \"\"\"\n{\n\t\"id\": \"0001\",\n\t\"type\": \"donut\",\n\t\"name\": \"Cake\",\n\t\"ppu\": 0.55,\n\t\"batters\":\n\t\t{\n\t\t\t\"batter\":\n\t\t\t\t[\n\t\t\t\t\t{ \"id\": \"1001\", \"type\": \"Regular\" },\n\t\t\t\t\t{ \"id\": \"1002\", \"type\": \"Chocolate\" },\n\t\t\t\t\t{ \"id\": \"1003\", \"type\": \"Blueberry\" },\n\t\t\t\t\t{ \"id\": \"1004\", \"type\": \"Devil's Food\" }\n\t\t\t\t]\n\t\t},\n\t\"topping\":\n\t\t[\n\t\t\t{ \"id\": \"5001\", \"type\": \"None\" },\n\t\t\t{ \"id\": \"5002\", \"type\": \"Glazed\" },\n\t\t\t{ \"id\": \"5005\", \"type\": \"Sugar\" },\n\t\t\t{ \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\n\t\t\t{ \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\n\t\t\t{ \"id\": \"5003\", \"type\": \"Chocolate\" },\n\t\t\t{ \"id\": \"5004\", \"type\": \"Maple\" }\n\t\t]\n}\n\"\"\"\n\nval rdd = sc.parallelize(Seq(jsonString))\nval df = snappy.read.json(rdd)\n\ndf.createOrReplaceTempView(\"arrayTable\") // Temp table is scoped to a single session(or single JDBC connection)\n\n/******\n** You use explode (equivalent to Lateral view in RDBs) to flatten Array or Map types.\n** And, we explicitly select the nested fields from a struct using the '.' notation. \n** Note that Topping is a Array of structs in the JSON\n******/\nval df1 = snappy.sql(\"select c1.id, c1.type from (select explode(topping) as c1 from arrayTable)t1\")\n\n/******\n** In above example, we created the RDD using the Json string. \n** But, if the Json data was externally stored we could also create a persistent view in the catalog. \n** The view will be accessible even when the cluster is bounced. \n******/\n\n// First write this sample data set to some file. \ndeleteFile(\"/tmp/temp100.json\") // delete if output file already present\ndf.write.json(\"/tmp/temp100.json\")\n\n//Create the external table and register in the ComputeDB catalog\nsnappy.sql(\"drop table if exists jsonT\")  //First, clean up\nsnappy.sql(\"create external table jsonT using json options(path '/tmp/temp100.json')\")\n\n// finally, create the view in the catalog\nsnappy.sql(\"drop view if exists jsonview\")\nsnappy.sql(\"create view jsonview as (select c1.id, c1.type from (select explode(topping) as c1 from jsonT))\")\n/******\n** The explode function flattens any Array or Map structured objects. Same as LATERAL VIEW SQL in other popular DBs\n** You could try to flatten the structure along with the built in Array using explode as an exercise. \n******/\n\nsnappy.table(\"jsonview\").printSchema","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"id":"string","type":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576643959870_1293455526","id":"20191104-233749_696372205","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18268"},{"title":"(3) Scala - Add, remove, rename columns ","text":"%snappydata.sql\nexec scala\n\nval df1 = df.withColumnRenamed(\"name\",\"ItemName\").withColumnRenamed(\"ppu\", \"PartsPerUnit\")\ndf1.printSchema\ndf1.show\nval df2 = df1.withColumn(\"comment\", col(\"ItemName\"))\nval df3 = df2.drop(\"comment\")\ndf3.printSchema","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"C0":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576643959871_2011827372","id":"20191104-234152_967535984","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18269"},{"title":"(4) SQL - Pivot a table","text":"%snappydata.sql\n\n-- Insert some test data into a table\ndrop table if exists avgTemp;\n-- We inject a 1000 rows using the range operator. \ncreate table avgTemp as (select (1900 + (id%100)) as year, (id%12)+1 as month, (rand()*100)as temp from range(1000));\n-- ComputeDB parser supports the PIVOT clause. It deviates from Spark 2.4 support in that it\n-- only allows literals in the value IN list rather than named expressions. On the contrary, it supports\n-- explicit GROUP BY columns with PIVOT instead of always doing implicit detection.\n\n  select * from (\n    select year, month, temp\n    from avgTemp\n  )\n  PIVOT (\n    CAST(avg(temp) AS DECIMAL(5, 2))\n    FOR month IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n  )\n  GROUP BY year\n  ORDER BY year DESC;","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"2":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"1":"string","2":"string","3":"string","4":"string","5":"string","6":"string","7":"string","8":"string","9":"string","10":"string","11":"string","12":"string","year":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576643959871_1929094987","id":"20191128-000723_639892178","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18270"},{"text":"%snappydata.sql\n","user":"anonymous","dateUpdated":"2019-12-18T10:09:19+0530","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576643959872_-498063854","id":"20191210-115149_487859028","dateCreated":"2019-12-18T10:09:19+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18271"}],"name":"Snappy/6. Data Transformation Examples","id":"2EXHWU3SV","noteParams":{},"noteForms":{},"angularObjects":{"angular:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}