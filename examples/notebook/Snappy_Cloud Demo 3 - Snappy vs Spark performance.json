{"paragraphs":[{"text":"%angular\n<script>\n    function openSnappyPulse(){\n      var proto = window.location.protocol;\n      var currHost = window.location.host;\n      var pulseUrl = proto + '//' + currHost.substring(0, currHost.indexOf(':') + 1) + '5050/dashboard/';\n      window.open(pulseUrl, \"_blank\");\n    }\n</script>\n\n<div style=\"background-color:whitesmoke;\">\n<br>\n <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span>\n<br>\n<a href=\"#/notebook/2ETF88QQF\">Zeppelin Notebook Manager</a>\n<br>\n    \n<h4>Quickstarts</h4>\n&ensp;<a href=\"#/notebook/quickstart\" >Using Spark Scala API</a>\n<br>\n&ensp;<a href=\"#/notebook/2EVF37179\" >Using SQL</a>\n<br>\n&ensp;<a href=\"#/notebook/performance\">Performance Benchmark</a>\n<br>\n\n<h4>External Data Sources </h4>\n&ensp;<a href=\"#/notebook/2EU4EJDHJ\">Load from External Data Sources</a>\n<br>\n&ensp;<a href=\"#/notebook/2EU6ZXZQJ\">Manage Connectors</a>\n<br>\n&ensp;<a href=\"#/notebook/2EUZ88BBY\">Data transformation examples</a>\n<br>\n\n<h4> Structured Streaming </h4>\n&ensp;<a href=\"#/notebook/2EUCAD6QP\">Example using file source</a>\n\n<h4><b> Demos with Big Datasets</b></h4>\n&ensp;<a href=\"#/notebook/airlineanalytics\">Airline Analytics Demo</a>\n<br>\n&ensp;<a href=\"#/notebook/nyctaxianalytics\">NYC Taxi Analytics Demo</a>\n<br>\n<!--&ensp;<a href=\"#/notebook/2DKKFJNZR\">Performance Benchmark(S3 dataset)</a>-->\n&ensp;<b><i>Performance Benchmark(S3 dataset)</i></b>\n<br>\n\n<h4>References</h4>\n&ensp;<a href=\"https://tibco-computedb.readthedocs.io/en/enterprise_docv1.2/\" target=\"_blank\">ComputeDB documentation</a>\n<br>\n&ensp;<a href=\"https://zeppelin.apache.org/docs/0.8.2/index.html\" target=\"_blank\">Zeppelin Documentation</a>\n</div>\n","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<script>\n    function openSnappyPulse(){\n      var proto = window.location.protocol;\n      var currHost = window.location.host;\n      var pulseUrl = proto + '//' + currHost.substring(0, currHost.indexOf(':') + 1) + '5050/dashboard/';\n      window.open(pulseUrl, \"_blank\");\n    }\n</script>\n\n<div style=\"background-color:whitesmoke;\">\n<br>\n <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span>\n<br>\n<a href=\"#/notebook/2ETF88QQF\">Zeppelin Notebook Manager</a>\n<br>\n    \n<h4>Quickstarts</h4>\n&ensp;<a href=\"#/notebook/quickstart\" >Using Spark Scala API</a>\n<br>\n&ensp;<a href=\"#/notebook/2EVF37179\" >Using SQL</a>\n<br>\n&ensp;<a href=\"#/notebook/performance\">Performance Benchmark</a>\n<br>\n\n<h4>External Data Sources </h4>\n&ensp;<a href=\"#/notebook/2EU4EJDHJ\">Load from External Data Sources</a>\n<br>\n&ensp;<a href=\"#/notebook/2EU6ZXZQJ\">Manage Connectors</a>\n<br>\n&ensp;<a href=\"#/notebook/2EUZ88BBY\">Data transformation examples</a>\n<br>\n\n<h4> Structured Streaming </h4>\n&ensp;<a href=\"#/notebook/2EUCAD6QP\">Example using file source</a>\n\n<h4><b> Demos with Big Datasets</b></h4>\n&ensp;<a href=\"#/notebook/airlineanalytics\">Airline Analytics Demo</a>\n<br>\n&ensp;<a href=\"#/notebook/nyctaxianalytics\">NYC Taxi Analytics Demo</a>\n<br>\n<!--&ensp;<a href=\"#/notebook/2DKKFJNZR\">Performance Benchmark(S3 dataset)</a>-->\n&ensp;<b><i>Performance Benchmark(S3 dataset)</i></b>\n<br>\n\n<h4>References</h4>\n&ensp;<a href=\"https://tibco-computedb.readthedocs.io/en/enterprise_docv1.2/\" target=\"_blank\">ComputeDB documentation</a>\n<br>\n&ensp;<a href=\"https://zeppelin.apache.org/docs/0.8.2/index.html\" target=\"_blank\">Zeppelin Documentation</a>\n</div>"}]},"apps":[],"jobName":"paragraph_1578908676106_550629825","id":"20191209-134659_1313693342","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:35526"},{"text":"%angular\n<div style=\"background-color:whitesmoke;\">\n<h2>What does this demo showcase?</h2>\nAnalytical queries on 170 million NYC taxi trip records.\n<p>The New York City Taxi & Limousine Commission has released the detailed historical dataset covering over 1.1 billion individual taxi trips in the city from January 2009 through June 2015. In this Analytics demo we only use data for Year 2013. Each trip contains information like the driver's license info, the pickup and dropoff times and their precise locations, etc. \nWe have uploaded the data set in parquet format in a public S3 bucket. \n\n<p> This simple benchmark runs adhoc analytical queries using Spark on the parquet data set. We load the data set into a Snappy Column table and run the exact same set of queries.</p>\n<h5 style=\"color:red;font-weight:bold\">Anonymous user needs to clone this notebook to execute the paragraphs. Clone option available in the pane next to name of the notebook at the top of the page.</h5>\n</div>","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/undefined","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<div style=\"background-color:whitesmoke;\">\n<h2>What does this demo showcase?</h2>\nAnalytical queries on 170 million NYC taxi trip records.\n<p>The New York City Taxi & Limousine Commission has released the detailed historical dataset covering over 1.1 billion individual taxi trips in the city from January 2009 through June 2015. In this Analytics demo we only use data for Year 2013. Each trip contains information like the driver's license info, the pickup and dropoff times and their precise locations, etc. \nWe have uploaded the data set in parquet format in a public S3 bucket. \n\n<p> This simple benchmark runs adhoc analytical queries using Spark on the parquet data set. We load the data set into a Snappy Column table and run the exact same set of queries.</p>\n<h5 style=\"color:red;font-weight:bold\">Anonymous user needs to clone this notebook to execute the paragraphs. Clone option available in the pane next to name of the notebook at the top of the page.</h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1578908676109_-606355551","id":"20160925-172803_1684905446","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35527"},{"title":"Monitor the cluster, check memory consumption","text":"%angular\n<div style=\"background-color:whitesmoke;\"> <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span> <span><h5> The Dashboard tab in the Monitoring Console can be used to monitor the cluster and check the memory consumed by the samples.</h5></span></div>\n","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/undefined","colWidth":10,"editorHide":true,"fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<div style=\"background-color:whitesmoke;\"> <span style=\"font-weight: bold; color:#428bca; cursor:pointer;\" onclick=\"openSnappyPulse();\">Monitoring Console </span> <span><h5> The Dashboard tab in the Monitoring Console can be used to monitor the cluster and check the memory consumed by the samples.</h5></span></div>"}]},"apps":[],"jobName":"paragraph_1578908676109_-846457818","id":"20170324-134252_1246434891","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35528"},{"text":"%spark\nz.put(\"aws_id\",z.textbox(\"AWS Access ID\"))\nz.put(\"aws_secret\",z.textbox(\"AWS Secret Key\"))\n\nprintln(s\"\"\"%angular\n <div style=\"color:red;background-color:whitesmoke;\">\n            <li><b>Access Credentials</b></li>\n            <ul>\n                <li>Please refer to security credentials guidelines <a href=\"https://tibco-computedb.readthedocs.io/en/enterprise_docv1.2/connectors/access_cloud_data/#using-hive-sitexml-file\" target=\"_blank\">here</a>.</li>\n                <li>The Zeppelin notebooks store state to disk. Using credentials via notebooks will result in them being written to disk. More about Zeppelin storage options is available <a href=\"https://zeppelin.apache.org/docs/0.8.0/setup/storage/storage.html\">here</a></li>\n                <li>If user still chooses to use notebooks as the mode of setting credentials, then user may clone this notebook and delete it immediately after completing use.</li>\n            </ul></div>\"\"\")\nz.run(\"20191209-143639_1295861470\")","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":10,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{"AWS Access ID":"","AWS Secret Key":""},"forms":{"AWS Access ID":{"type":"TextBox","name":"AWS Access ID","displayName":"AWS Access ID","defaultValue":"","hidden":false,"$$hashKey":"object:35696"},"AWS Secret Key":{"type":"TextBox","name":"AWS Secret Key","displayName":"AWS Secret Key","defaultValue":"","hidden":false,"$$hashKey":"object:35697"}}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":" <div style=\"color:red;background-color:whitesmoke;\">\n            <li><b>Access Credentials</b></li>\n            <ul>\n                <li>Please refer to security credentials guidelines <a href=\"https://tibco-computedb.readthedocs.io/en/enterprise_docv1.2/connectors/access_cloud_data/#using-hive-sitexml-file\" target=\"_blank\">here</a>.</li>\n                <li>The Zeppelin notebooks store state to disk. Using credentials via notebooks will result in them being written to disk. More about Zeppelin storage options is available <a href=\"https://zeppelin.apache.org/docs/0.8.0/setup/storage/storage.html\">here</a></li>\n                <li>If user still chooses to use notebooks as the mode of setting credentials, then user may clone this notebook and delete it immediately after completing use.</li>\n            </ul></div>\n"}]},"apps":[],"jobName":"paragraph_1578908676109_-1416300094","id":"20191209-140826_1616777192","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35529"},{"title":"[ auto-run ]","text":"%jdbc \nexec scala\norg.apache.hadoop.fs.FileSystem.closeAll()\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"{aws_id}\")\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"{aws_secret}\")","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":true,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"C0":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"C0\n"}]},"apps":[],"jobName":"paragraph_1578908676109_-1475817712","id":"20191209-143639_1295861470","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35530"},{"title":"Define a function to measure the timings","text":"%jdbc\nexec scala\n\ndef benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: => Unit) {\n  for (i <- 1 to warmups) {\n    f\n  }\n  val startTime = System.nanoTime\n  for (i <- 1 to times) {\n    f\n  }\n  val endTime = System.nanoTime\n  println(s\"Average time taken in $name for $times runs: \" +\n  (endTime - startTime).toDouble / (times * 1000000.0) + \" ms\")\n}","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":true,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"C0":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"C0\nbenchmark: (name: String, times: Int, warmups: Int)(f: => Unit)Unit\n"}]},"apps":[],"jobName":"paragraph_1578908676110_-822734189","id":"20180702-043803_402706850","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35531"},{"title":"Load the snappy column table (Note: This will pull data from AWS storage to your cluster !! Recommended to launch this notebook in AWS instance.)","text":"%jdbc\nexec scala\n\n// WARNING: MAKE SURE YOU HAVE CONFIGURED ENOUGH MEMORY IN YOUR DATA STORE SERVERS. \n\nval df1 = snappysession.read.parquet(\"s3a://computedb-test-data/nytaxitripdata_cleaned\")\n\n// You may want to set options like Number of buckets ('buckets') and partitioning key ...\n// e.g. df1.write.format(\"column\").mode(\"overwrite\").option(\"buckets\", \"33\").option(\"partition_by\", \"medallion\").saveAsTable(\"nyc_taxi_trip\")\ndf1.write.format(\"column\").saveAsTable(\"nyc_taxi_trip\")\ndf1.printSchema","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578908676110_1717233290","id":"20180702-043929_1783065412","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35532"},{"title":"Lets run a few queries using Snappy Session","text":"%jdbc\nexec scala\nbenchmark(\"Q1\",1,0) { snappysession.sql(\"select count(*) from nyc_taxi_trip\").collect } \n\n//Number of rides per week\nbenchmark(\"Q2\",1,0){ snappysession.sql(\"select count(*) numOfRides, weekofyear(pickup_datetime) from nyc_taxi_trip group by weekofyear(pickup_datetime)\").collect }\n\n//Top 50 drivers\nbenchmark(\"Q3\",1,0){ snappysession.sql(\"select count(*) numOfRides, hack_license from nyc_taxi_trip group by hack_license order by count(*) desc limit 50\").collect }\n\n//Top 50 drivers by distance\nbenchmark(\"Q4\",1,0){ snappysession.sql(\"select sum(trip_distance) total_distance, hack_license from nyc_taxi_trip group by hack_license order by sum(trip_distance) desc limit 50\").collect }\n\n//------ Lets run it again ------\nprintln(\"========= Second run =========\")\nbenchmark(\"Q1\",1,0) { snappysession.sql(\"select count(*) from nyc_taxi_trip\").collect } \n\n//Number of rides per week\nbenchmark(\"Q2\",1,0){ snappysession.sql(\"select count(*) numOfRides, weekofyear(pickup_datetime) from nyc_taxi_trip group by weekofyear(pickup_datetime)\").collect }\n\n//Top 50 drivers\nbenchmark(\"Q3\",1,0){ snappysession.sql(\"select count(*) numOfRides, hack_license from nyc_taxi_trip group by hack_license order by count(*) desc limit 50\").collect }\n\n//Top 50 drivers by distance\nbenchmark(\"Q4\",1,0){ snappysession.sql(\"select sum(trip_distance) total_distance, hack_license from nyc_taxi_trip group by hack_license order by sum(trip_distance) desc limit 50\").collect }\n\n","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578908676110_-1716336628","id":"20180702-044303_935075656","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35533"},{"title":"NOW RUN USING SPARK ....","text":"%angular\n<h4>Configure your Spark interpreter to point to the correct Spark master. By default it will use local[*] and may run out of memory.</h4>","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<h4>Configure your Spark interpreter to point to the correct Spark master. By default it will use local[*] and may run out of memory.</h4>"}]},"apps":[],"jobName":"paragraph_1578908676110_-1840286758","id":"20180702-044752_2087842369","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35534"},{"title":"Define a function to measure the timings using Spark interpreter also","text":"%jdbc\nexec scala\ndef benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: => Unit) {\n    for (i <- 1 to warmups) {\n        f\n    }\n    val startTime = System.nanoTime\n    for (i <- 1 to times) {\n        f\n    }\n    val endTime = System.nanoTime\n    println(s\"Average time taken in $name for $times runs: \" +\n    (endTime - startTime).toDouble / (times * 1000000.0) + \" ms\")\n}\n\norg.apache.spark.sql.SparkSession.clearActiveSession() // clear the existing SnappySession\nval spark = org.apache.spark.sql.SparkSession.builder().appName(\"SnappyBenchmark\").getOrCreate()","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578908676111_789503530","id":"20180702-045051_1560429760","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35535"},{"title":"Load the S3 parquet data set into Spark","text":"%jdbc\nexec scala\nimport org.apache.spark.sql._\n\nval df1 = spark.read.parquet(\"s3a://computedb-test-data/nytaxitripdata_cleaned\")\ndf1.cache\ndf1.createOrReplaceTempView(\"nyc_taxi_trip\")\n","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578908676111_-864955600","id":"20180702-044550_1824705584","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35536"},{"title":"Lets run a few queries using Spark Session","text":"%jdbc\nexec scala\nbenchmark(\"Q1\",1,0) { spark.sql(\"select count(*) from nyc_taxi_trip\").collect } \n\n//Number of rides per week\nbenchmark(\"Q2\",1,0){ spark.sql(\"select count(*) numOfRides, weekofyear(pickup_datetime) from nyc_taxi_trip group by weekofyear(pickup_datetime)\").collect }\n\n//Top 50 drivers\nbenchmark(\"Q3\",1,0){ spark.sql(\"select count(*) numOfRides, hack_license from nyc_taxi_trip group by hack_license order by count(*) desc limit 50\").collect }\n\n//Top 50 drivers by distance\nbenchmark(\"Q4\",1,0){ spark.sql(\"select sum(trip_distance) total_distance, hack_license from nyc_taxi_trip group by hack_license order by sum(trip_distance) desc limit 50\").collect }\n\n//------ Lets run it again ------\nprintln(\"========= Second run =========\")\nbenchmark(\"Q1\",1,0) { spark.sql(\"select count(*) from nyc_taxi_trip\").collect } \n\n//Number of rides per week\nbenchmark(\"Q2\",1,0){ spark.sql(\"select count(*) numOfRides, weekofyear(pickup_datetime) from nyc_taxi_trip group by weekofyear(pickup_datetime)\").collect }\n\n//Top 50 drivers\nbenchmark(\"Q3\",1,0){ spark.sql(\"select count(*) numOfRides, hack_license from nyc_taxi_trip group by hack_license order by count(*) desc limit 50\").collect }\n\n//Top 50 drivers by distance\nbenchmark(\"Q4\",1,0){ spark.sql(\"select sum(trip_distance) total_distance, hack_license from nyc_taxi_trip group by hack_license order by sum(trip_distance) desc limit 50\").collect }","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578908676111_-860481133","id":"20180702-044820_1788487692","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35537"},{"text":"%angular\n<div style=\"background-color:whitesmoke;\">\n<h2> Try your own queries below ....</h2>\n<h4>You can always share your thoughts/questions or just encourage us at <a href=\"https://www.tibco.com/products/tibco-computedb\" target=\"_blank\"> https://www.tibco.com/products/tibco-computedb </a></h4>\n(You can use <a href=\"http://snappydata-slackin.herokuapp.com/\" target=\"_blank\">slack</a>,<a href=\"https://gitter.im/SnappyDataInc/snappydata\" target=\"_blank\">Gitter</a>,<a href=\"http://stackoverflow.com/questions/tagged/snappydata\" target=\"_blank\">stackoverflow </a>, or <a href=\"https://groups.google.com/forum/#!forum/snappydata-user\" target=\"_blank\">google groups</a>)\n</div>","user":"anonymous","dateUpdated":"2020-01-13T15:14:36+0530","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<div style=\"background-color:whitesmoke;\">\n<h2> Try your own queries below ....</h2>\n<h4>You can always share your thoughts/questions or just encourage us at <a href=\"https://www.tibco.com/products/tibco-computedb\" target=\"_blank\"> https://www.tibco.com/products/tibco-computedb </a></h4>\n(You can use <a href=\"http://snappydata-slackin.herokuapp.com/\" target=\"_blank\">slack</a>,<a href=\"https://gitter.im/SnappyDataInc/snappydata\" target=\"_blank\">Gitter</a>,<a href=\"http://stackoverflow.com/questions/tagged/snappydata\" target=\"_blank\">stackoverflow </a>, or <a href=\"https://groups.google.com/forum/#!forum/snappydata-user\" target=\"_blank\">google groups</a>)\n</div>"}]},"apps":[],"jobName":"paragraph_1578908676111_1971249326","id":"20170324-134332_252144791","dateCreated":"2020-01-13T15:14:36+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35538"}],"name":"Snappy/Cloud Demo 3 - Snappy vs Spark performance","id":"2F1EBDQDA","noteParams":{},"noteForms":{},"angularObjects":{"angular:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
